{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T02:04:19.360670Z",
     "start_time": "2025-08-08T02:04:19.350573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T02:04:19.384056Z",
     "start_time": "2025-08-08T02:04:19.376751Z"
    }
   },
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gridworld import GridWorldEnv\n",
    "from agent import PolicyIterationAgent"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T02:04:19.402323Z",
     "start_time": "2025-08-08T02:04:19.395640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = GridWorldEnv()\n",
    "env.reset()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T02:04:19.418672Z",
     "start_time": "2025-08-08T02:04:19.412682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 0.9 is the agent discount factor\n",
    "pi_agent = PolicyIterationAgent(0.9)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration Agent\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T02:04:19.439976Z",
     "start_time": "2025-08-08T02:04:19.429138Z"
    }
   },
   "source": "V_optimal, pi_optimal = pi_agent.policyIterate()",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Policy Iteration...\n",
      "\n",
      "=== Iteration 1 ===\n",
      "Policy Evaluation converged in 30 iterations (delta = 0.000905)\n",
      "State Values after Policy Evaluation:\n",
      "Value Function--------------------------\n",
      "array([[ 0.        , -2.12753316, -5.21106469, -6.08103978],\n",
      "       [-2.12753316, -4.34147004, -5.29864063, -5.21172108],\n",
      "       [-5.21106469, -5.29864063, -4.34199516, -2.12838959],\n",
      "       [-6.08103978, -5.21172108, -2.12838959,  0.        ]])\n",
      "\n",
      "\n",
      "Policy after Policy Improvement:\n",
      "Policy--------------------------\n",
      "[['T', '←', '←', '←'],\n",
      " ['↑', '↑', '←', '↓'],\n",
      " ['↑', '↑', '→', '↓'],\n",
      " ['↑', '→', '→', 'T']]\n",
      "\n",
      "\n",
      "\n",
      "=== Iteration 2 ===\n",
      "Policy Evaluation converged in 3 iterations (delta = 0.000000)\n",
      "State Values after Policy Evaluation:\n",
      "Value Function--------------------------\n",
      "array([[0.  , 5.  , 3.5 , 2.15],\n",
      "       [5.  , 3.5 , 2.15, 3.5 ],\n",
      "       [3.5 , 2.15, 3.5 , 5.  ],\n",
      "       [2.15, 3.5 , 5.  , 0.  ]])\n",
      "\n",
      "\n",
      "Policy after Policy Improvement:\n",
      "Policy--------------------------\n",
      "[['T', '←', '←', '↓'],\n",
      " ['↑', '↑', '↑', '↓'],\n",
      " ['↑', '↑', '→', '↓'],\n",
      " ['↑', '→', '→', 'T']]\n",
      "\n",
      "\n",
      "\n",
      "=== Iteration 3 ===\n",
      "Policy Evaluation converged in 1 iterations (delta = 0.000000)\n",
      "State Values after Policy Evaluation:\n",
      "Value Function--------------------------\n",
      "array([[0.  , 5.  , 3.5 , 2.15],\n",
      "       [5.  , 3.5 , 2.15, 3.5 ],\n",
      "       [3.5 , 2.15, 3.5 , 5.  ],\n",
      "       [2.15, 3.5 , 5.  , 0.  ]])\n",
      "\n",
      "\n",
      "Policy after Policy Improvement:\n",
      "Policy--------------------------\n",
      "[['T', '←', '←', '↓'],\n",
      " ['↑', '↑', '↑', '↓'],\n",
      " ['↑', '↑', '→', '↓'],\n",
      " ['↑', '→', '→', 'T']]\n",
      "\n",
      "\n",
      "Policy is stable! Convergence achieved.\n",
      "\n",
      "Policy Iteration converged in 3 iterations!\n",
      "Final Results:\n",
      "==============\n",
      "Value Function--------------------------\n",
      "array([[0.  , 5.  , 3.5 , 2.15],\n",
      "       [5.  , 3.5 , 2.15, 3.5 ],\n",
      "       [3.5 , 2.15, 3.5 , 5.  ],\n",
      "       [2.15, 3.5 , 5.  , 0.  ]])\n",
      "\n",
      "\n",
      "Policy--------------------------\n",
      "[['T', '←', '←', '↓'],\n",
      " ['↑', '↑', '↑', '↓'],\n",
      " ['↑', '↑', '→', '↓'],\n",
      " ['↑', '→', '→', 'T']]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T02:04:19.448527Z",
     "start_time": "2025-08-08T02:04:19.442745Z"
    }
   },
   "cell_type": "code",
   "source": "num_episodes = 6",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T02:04:19.469463Z",
     "start_time": "2025-08-08T02:04:19.462895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    max_steps = 20  # Prevent infinite loops\n",
    "\n",
    "    print(f\"\\nEpisode {episode + 1}:\")\n",
    "    print(f\"Starting state: {state}\")\n",
    "\n",
    "    while steps < max_steps:\n",
    "        # Find the state object corresponding to current position\n",
    "        current_state = None\n",
    "        for s in pi_agent.S:\n",
    "            if s.coord == state:\n",
    "                current_state = s\n",
    "                break\n",
    "\n",
    "        if current_state is None or current_state.isTerminal():\n",
    "            break\n",
    "\n",
    "        # Get action from policy (choose action with highest probability)\n",
    "        action = np.argmax(pi_optimal[current_state])\n",
    "\n",
    "        # Take action in environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        action_names = ['Up', 'Right', 'Down', 'Left']\n",
    "        print(f\"  Step {steps}: {state} -> {action_names[action]} -> {next_state}, Reward: {reward}\")\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(f\"  Episode finished! Total reward: {total_reward}, Steps: {steps}\")\n",
    "            break\n",
    "\n",
    "    if steps >= max_steps:\n",
    "        print(f\"  Episode terminated after {max_steps} steps (max limit reached)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 1:\n",
      "Starting state: (0, 3)\n",
      "  Step 1: (0, 3) -> Down -> (1, 3), Reward: -1\n",
      "  Step 2: (1, 3) -> Down -> (2, 3), Reward: -1\n",
      "  Step 3: (2, 3) -> Down -> (3, 3), Reward: 5\n",
      "  Episode finished! Total reward: 3, Steps: 3\n",
      "\n",
      "Episode 2:\n",
      "Starting state: (0, 3)\n",
      "  Step 1: (0, 3) -> Down -> (1, 3), Reward: -1\n",
      "  Step 2: (1, 3) -> Down -> (2, 3), Reward: -1\n",
      "  Step 3: (2, 3) -> Down -> (3, 3), Reward: 5\n",
      "  Episode finished! Total reward: 3, Steps: 3\n",
      "\n",
      "Episode 3:\n",
      "Starting state: (0, 3)\n",
      "  Step 1: (0, 3) -> Down -> (1, 3), Reward: -1\n",
      "  Step 2: (1, 3) -> Down -> (2, 3), Reward: -1\n",
      "  Step 3: (2, 3) -> Down -> (3, 3), Reward: 5\n",
      "  Episode finished! Total reward: 3, Steps: 3\n",
      "\n",
      "Episode 4:\n",
      "Starting state: (0, 3)\n",
      "  Step 1: (0, 3) -> Down -> (1, 3), Reward: -1\n",
      "  Step 2: (1, 3) -> Down -> (2, 3), Reward: -1\n",
      "  Step 3: (2, 3) -> Down -> (3, 3), Reward: 5\n",
      "  Episode finished! Total reward: 3, Steps: 3\n",
      "\n",
      "Episode 5:\n",
      "Starting state: (0, 3)\n",
      "  Step 1: (0, 3) -> Down -> (1, 3), Reward: -1\n",
      "  Step 2: (1, 3) -> Down -> (2, 3), Reward: -1\n",
      "  Step 3: (2, 3) -> Down -> (3, 3), Reward: 5\n",
      "  Episode finished! Total reward: 3, Steps: 3\n",
      "\n",
      "Episode 6:\n",
      "Starting state: (0, 3)\n",
      "  Step 1: (0, 3) -> Down -> (1, 3), Reward: -1\n",
      "  Step 2: (1, 3) -> Down -> (2, 3), Reward: -1\n",
      "  Step 3: (2, 3) -> Down -> (3, 3), Reward: 5\n",
      "  Episode finished! Total reward: 3, Steps: 3\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
